Project Design. 
This project uses 2 different machine learning models in Python to predict unemployment outcomes (1 = unemployed, 0 = employed) for clients who received services from the U.S. Department of Labor (DOL)  4 quarters after they exited workforce system services. I selected two different tree-based machine learning algorithms: Decision Trees and Random Forests. Decision Trees have the benefit of simplicity and easy to explain to broad audiences but the disadvantages of often over-fitting the data. Random Forests are more complex in structure and far more computationally intensive but produce more robust results that generalize better to unseen data. In machine learning, we call the predictive variables “features.” Tree based models require very little feature pre-processing. For example, you do not need to normalize or standardize the features to be on the same scale. My features include participant demographics and socioeconomic characteristics, variables that reflect their workforce experience (duration in days, number of spells of service), and skills ratings based on their most recent occupation.  I evaluated how well each of the algorithms predicted my “unseen” test data using a measure of “accuracy,” that is the fraction of correctly classified samples. I referenced a textbook An Introduction to Machine Learning with Python by Andreas C. Muller and Sarah Guido (O’Reilly). Copyright 2017 Sarah Guido and Andreas Muller, 978-1-449-36941-5” for code examples and to understand the basic steps of implementing the tree-based models.
YouTube 2-minute Video Link:
https://youtu.be/RgxPitHmpoY
Project Website. 
https://jhsmith22.github.io/workforce_ml/
I also create a project website using GitHub Pages that contains the following:
1.	My embedded 2-min YouTube video
2.	An embedded lengthier set of slides that more fully describes my data, methods, and results
3.	A link to my Github page with the Python code
4.	A link to the zipped csv file required to feed into my Python code to replicate my results.

Data. 
The two data sources are “PY 2018 Q2 WIOA Performance Records Public Use Data File” (https://www.dol.gov/agencies/eta/performance/results/pirl)  and the “O*Net Skills Database” (https://www.onetcenter.org/dictionary/25.2/excel/skills.html), merged together on the SOC code. The  WIOA records include participant characteristics, information about services received, and employment outcomes. The O*Net Skills Database includes a rating on 35 skills (e.g., Reading Comprehension, Active Listening, Writing) for each SOC code, from which I created 35 features (e.g., Chief Executives have a score of 4.88 on Active Listening and a score of 0 on Equipment Maintenance). I performed several pre-processing steps to reduce the size of the huge data file (20 million records) prior to loading the file into my Python code so the file could be reasonably used by my Python program and other users. These included removing about 10 million participants who lacked data on unemployment outcomes (because they had exited workforce center services less than 1 year prior), filtering the data to those age 25-65, and merging in the O*NET Skills Database columns on SOC code. The analytic dataset loaded into Python is about 4 million rows.
Analytic Steps. 
1. Engineer the features, cleaning data and recoding values as needed. This included 4 key steps. (1) One was de-duplicating my data. Some participants were represented multiple times in the data because every row represented a different spell of service receipt. After first creating new variables that represented the total days of service receipt across all spells of service and a count of the number of spells of service, I dropped all rows except the most recent spell of service. I then had only 1 record per participant. (2) I also recoded some features to have more predictive power. For example, I combined the “high school diploma” and “GED” categories together in the educational attainment variable because these values represent the same basic level of education. (3) I also converted all categorical variables to binary dummy variables (e.g., race, age categories, state) as this is necessary to use them in my machine learning algorithms, removing one of the dummy categories from each. (4) After data cleaning, I removed all rows that are missing (NaN values) for any of the features. I notably lost about half my rows due to missing data on the Skills Ratings features, which threatens the generalizability of findings to the full workforce system population.
2. Split data into a training (75% of data) and test set (25% of data). I also leveraged 5-fold cross-validation to further split the training dataset into 5 equal parts for robust testing. With 5-fold cross-validation, you alternate which folds represent the training dataset and which fold is the validation dataset. For example, you train the model on folds 2-5 and check how accurately the model parameters predict the outcomes of fold #1 (the validation dataset). The accuracy measure is the fraction of correctly classified rows as either employed or unemployed. You repeat this process, so that each of the folds has a turn representing the validation dataset. I assessed the model’s fit on the average accuracy across all 5 validation datasets.
3. Try out a range of parameters. For Decision Trees, the parameter I tried was the maximum number of leaf node (max_leaf_nodes as follows: 4, 10, 20, 30, 40, 40, 50, 60, 100*). For the Random Forest, I varied 3 three different parameters: the maximum depth of the trees (max_depth as follows: 4, 10*, 30, 50), the number of trees (n_estimators as follows: 100*, 200), and the number of features that are considered for building each tree (max_features as follows: 10, 14). I set the max features parameter options to be roughly equal to the square root of the n_estimators parameter. 
4. Pick the final model. I selected the parameters for the Decision Tree and Random Forest models that most accurately predicted the outcomes in the validation datasets. These are the final model parameter selections are indicated with as asterisk* in #3 above.
5. Assess how well your model generalizes to unseen data. In other words, I evaluated how accurately my final Decision Tree and Random Forest models predicts the outcomes in the “test” dataset, using the same accuracy measure. I found both my best Decision Tree model and my best Random Forest model predicted the outcome with a 67% accuracy. 

